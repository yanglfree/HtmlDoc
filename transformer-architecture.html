<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer æ¶æ„è¯¦è§£ - å¿«é€ŸæŸ¥è¯¢æ‰‹å†Œ</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0d1117;
            --bg2: #161b22;
            --bg3: #21262d;
            --text: #e6edf3;
            --text2: #8b949e;
            --blue: #58a6ff;
            --green: #3fb950;
            --purple: #a371f7;
            --orange: #d29922;
            --cyan: #39c5cf;
            --border: #30363d
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box
        }

        body {
            font-family: 'Inter', system-ui, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 24px
        }

        .hero {
            text-align: center;
            padding: 48px 24px;
            margin-bottom: 48px;
            background: linear-gradient(135deg, rgba(88, 166, 255, 0.15), rgba(57, 197, 207, 0.15));
            border-radius: 16px;
            border: 1px solid var(--border)
        }

        .hero h1 {
            font-size: 34px;
            margin-bottom: 12px;
            background: linear-gradient(135deg, var(--blue), var(--cyan));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent
        }

        .hero p {
            color: var(--text2);
            font-size: 16px
        }

        .section {
            margin-bottom: 48px
        }

        .section h2 {
            font-size: 22px;
            margin-bottom: 20px;
            padding-bottom: 12px;
            border-bottom: 2px solid var(--border);
            display: flex;
            align-items: center;
            gap: 10px
        }

        .section h2::before {
            content: '';
            width: 4px;
            height: 22px;
            background: linear-gradient(var(--blue), var(--cyan));
            border-radius: 2px
        }

        .section h3 {
            font-size: 18px;
            margin: 24px 0 16px;
            color: var(--blue)
        }

        p {
            margin-bottom: 16px;
            color: var(--text2)
        }

        code {
            font-family: 'JetBrains Mono', monospace;
            background: var(--bg3);
            padding: 2px 8px;
            border-radius: 6px;
            font-size: 13px;
            color: var(--cyan)
        }

        pre {
            background: var(--bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 20px;
            overflow-x: auto;
            font-size: 13px;
            line-height: 1.8;
            margin: 16px 0
        }

        pre code {
            background: none;
            padding: 0;
            color: var(--text)
        }

        .comment {
            color: var(--text2)
        }

        .keyword {
            color: var(--purple)
        }

        .string {
            color: var(--green)
        }

        .number {
            color: var(--orange)
        }

        .cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
            margin: 20px 0
        }

        .card {
            background: var(--bg2);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
            transition: all .2s
        }

        .card:hover {
            border-color: var(--blue);
            transform: translateY(-2px)
        }

        .card h3 {
            font-size: 16px;
            color: var(--blue);
            margin-bottom: 12px
        }

        .card p,
        .card li {
            font-size: 14px;
            color: var(--text2);
            margin: 0
        }

        .card ul {
            margin: 8px 0 0 16px
        }

        .tip {
            background: rgba(88, 166, 255, 0.1);
            border: 1px solid rgba(88, 166, 255, 0.3);
            border-radius: 12px;
            padding: 16px 20px;
            margin: 20px 0
        }

        .tip.success {
            background: rgba(63, 185, 80, 0.1);
            border-color: rgba(63, 185, 80, 0.3)
        }

        .tip.warn {
            background: rgba(210, 153, 34, 0.1);
            border-color: rgba(210, 153, 34, 0.3)
        }

        .tip strong {
            display: block;
            margin-bottom: 6px;
            color: var(--blue)
        }

        .tip.success strong {
            color: var(--green)
        }

        .tip.warn strong {
            color: var(--orange)
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 14px;
            background: var(--bg2);
            border-radius: 12px;
            overflow: hidden;
            border: 1px solid var(--border);
            margin: 16px 0
        }

        th {
            background: var(--bg3);
            padding: 14px 16px;
            text-align: left;
            font-weight: 600
        }

        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border);
            vertical-align: top
        }

        tr:last-child td {
            border-bottom: none
        }

        .formula {
            background: var(--bg3);
            border-radius: 8px;
            padding: 16px 20px;
            font-family: 'JetBrains Mono', monospace;
            font-size: 14px;
            text-align: center;
            margin: 16px 0;
            color: var(--cyan)
        }

        .arch {
            background: var(--bg2);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 24px;
            margin: 20px 0
        }

        .arch-title {
            font-size: 14px;
            color: var(--text2);
            margin-bottom: 16px;
            text-transform: uppercase;
            letter-spacing: 1px
        }

        .arch-box {
            display: flex;
            flex-direction: column;
            gap: 8px;
            align-items: center
        }

        .arch-layer {
            background: var(--bg3);
            border-radius: 8px;
            padding: 12px 24px;
            text-align: center;
            width: 100%;
            max-width: 300px
        }

        .arch-layer.highlight {
            background: rgba(88, 166, 255, 0.2);
            border: 1px solid var(--blue)
        }

        .arch-arrow {
            color: var(--blue);
            font-size: 20px
        }

        .toc {
            position: fixed;
            top: 40px;
            right: 40px;
            width: 200px;
            background: var(--bg2);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 16px
        }

        .toc h3 {
            font-size: 12px;
            color: var(--text2);
            text-transform: uppercase;
            margin-bottom: 12px
        }

        .toc a {
            display: block;
            padding: 6px 0;
            color: var(--text2);
            text-decoration: none;
            font-size: 13px
        }

        .toc a:hover {
            color: var(--blue)
        }

        @media(max-width:1200px) {
            .toc {
                display: none
            }
        }

        @media(max-width:768px) {
            .container {
                padding: 24px 16px
            }

            .hero h1 {
                font-size: 26px
            }

            .cards {
                grid-template-columns: 1fr
            }
        }
    </style>
</head>

<body>
    <nav class="toc">
        <h3>ç›®å½•</h3><a href="#overview">æ¶æ„æ€»è§ˆ</a><a href="#attention">æ³¨æ„åŠ›æœºåˆ¶</a><a href="#encoder">ç¼–ç å™¨</a><a
            href="#decoder">è§£ç å™¨</a><a href="#components">æ ¸å¿ƒç»„ä»¶</a><a href="#math">æ•°å­¦åŸç†</a><a href="#variants">å˜ä½“æ¶æ„</a><a
            href="#code">ä»£ç å®ç°</a>
    </nav>

    <div class="container">
        <div class="hero">
            <h1>ğŸ”® Transformer æ¶æ„è¯¦è§£</h1>
            <p>Attention Is All You Need - æ·±åº¦å­¦ä¹ é©å‘½æ€§æ¶æ„</p>
        </div>

        <section id="overview" class="section">
            <h2>æ¶æ„æ€»è§ˆ</h2>
            <p>Transformer æ˜¯ 2017 å¹´ Google åœ¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ä¸­æå‡ºçš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒäº† RNN/LSTM
                çš„å¾ªç¯ç»“æ„ï¼Œå®ç°äº†é«˜åº¦å¹¶è¡ŒåŒ–ã€‚</p>

            <div class="cards">
                <div class="card">
                    <h3>ğŸ¯ æ ¸å¿ƒåˆ›æ–°</h3>
                    <p>Self-Attention æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªä½ç½®æ—¶å…³æ³¨è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®</p>
                </div>
                <div class="card">
                    <h3">âš¡ ä¸»è¦ä¼˜åŠ¿</h3>
                        <p>é«˜åº¦å¹¶è¡ŒåŒ–è®­ç»ƒã€é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ã€å¯è§£é‡Šæ€§å¼º</p>
                </div>
                <div class="card">
                    <h3">ğŸŒ å¹¿æ³›åº”ç”¨</h3>
                        <p>GPTã€BERTã€ViTã€DALL-E ç­‰ç°ä»£ AI æ¨¡å‹çš„åŸºç¡€</p>
                </div>
            </div>

            <div class="arch">
                <div class="arch-title">Transformer æ•´ä½“æ¶æ„</div>
                <div style="display:flex;gap:40px;justify-content:center;flex-wrap:wrap">
                    <div class="arch-box">
                        <div style="font-size:14px;color:var(--text2);margin-bottom:8px">ç¼–ç å™¨ (Encoder)</div>
                        <div class="arch-layer">Output Probabilities</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer">Feed Forward</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer highlight">Multi-Head Attention</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer">Positional Encoding</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer">Input Embedding</div>
                    </div>
                    <div class="arch-box">
                        <div style="font-size:14px;color:var(--text2);margin-bottom:8px">è§£ç å™¨ (Decoder)</div>
                        <div class="arch-layer">Output Probabilities</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer">Feed Forward</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer highlight">Cross Attention</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer highlight">Masked Self-Attention</div>
                        <div class="arch-arrow">â†‘</div>
                        <div class="arch-layer">Output Embedding</div>
                    </div>
                </div>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>ç»„ä»¶</th>
                        <th>ä½œç”¨</th>
                        <th>å…³é”®ç‰¹ç‚¹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Embedding</strong></td>
                        <td>å°†è¯è½¬æ¢ä¸ºå‘é‡</td>
                        <td>å¯å­¦ä¹ çš„è¯å‘é‡è¡¨</td>
                    </tr>
                    <tr>
                        <td><strong>Positional Encoding</strong></td>
                        <td>ç¼–ç ä½ç½®ä¿¡æ¯</td>
                        <td>æ­£å¼¦/ä½™å¼¦å‡½æ•°æˆ–å¯å­¦ä¹ </td>
                    </tr>
                    <tr>
                        <td><strong>Multi-Head Attention</strong></td>
                        <td>å¹¶è¡Œæ³¨æ„åŠ›è®¡ç®—</td>
                        <td>å¤šä¸ªæ³¨æ„åŠ›å¤´æ•æ‰ä¸åŒç‰¹å¾</td>
                    </tr>
                    <tr>
                        <td><strong>Feed Forward</strong></td>
                        <td>éçº¿æ€§å˜æ¢</td>
                        <td>ä¸¤å±‚å…¨è¿æ¥ + ReLU/GELU</td>
                    </tr>
                    <tr>
                        <td><strong>Layer Norm</strong></td>
                        <td>å½’ä¸€åŒ–</td>
                        <td>ç¨³å®šè®­ç»ƒ</td>
                    </tr>
                    <tr>
                        <td><strong>Residual</strong></td>
                        <td>æ®‹å·®è¿æ¥</td>
                        <td>ç¼“è§£æ¢¯åº¦æ¶ˆå¤±</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="attention" class="section">
            <h2>æ³¨æ„åŠ›æœºåˆ¶ (Attention)</h2>
            <p>æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒï¼Œå®ƒè®¡ç®—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ ä¸å…¶ä»–æ‰€æœ‰å…ƒç´ çš„ç›¸å…³æ€§ã€‚</p>

            <h3>Scaled Dot-Product Attention</h3>
            <div class="formula">Attention(Q, K, V) = softmax(QK<sup>T</sup> / âˆšd<sub>k</sub>) Â· V</div>

            <div class="cards">
                <div class="card">
                    <h3">ğŸ”‘ Query (Q)</h3>
                        <p>æŸ¥è¯¢å‘é‡ï¼Œå½“å‰ä½ç½®æƒ³è¦è·å–çš„ä¿¡æ¯</p>
                </div>
                <div class="card">
                    <h3">ğŸ” Key (K)</h3>
                        <p>é”®å‘é‡ï¼Œå…¶ä»–ä½ç½®æä¾›çš„ç´¢å¼•ä¿¡æ¯</p>
                </div>
                <div class="card">
                    <h3">ğŸ’ Value (V)</h3>
                        <p>å€¼å‘é‡ï¼Œå…¶ä»–ä½ç½®å®é™…ä¼ é€’çš„ä¿¡æ¯</p>
                </div>
            </div>

            <h3>è®¡ç®—æµç¨‹</h3>
            <pre><code><span class="comment"># Step 1: è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯</span>
scores = Q @ K.T  <span class="comment"># [seq_len, seq_len]</span>

<span class="comment"># Step 2: ç¼©æ”¾ (é˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±)</span>
scores = scores / sqrt(d_k)

<span class="comment"># Step 3: Softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡</span>
attention_weights = softmax(scores)

<span class="comment"># Step 4: åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º</span>
output = attention_weights @ V</code></pre>

            <h3>Multi-Head Attention</h3>
            <p>ä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œè®¡ç®—ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼š</p>

            <div class="formula">MultiHead(Q, K, V) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>) W<sup>O</sup>
            </div>

            <pre><code><span class="comment"># å…¸å‹é…ç½® (ä»¥ GPT-3 ä¸ºä¾‹)</span>
d_model = <span class="number">12288</span>     <span class="comment"># æ¨¡å‹ç»´åº¦</span>
n_heads = <span class="number">96</span>         <span class="comment"># æ³¨æ„åŠ›å¤´æ•°é‡</span>
d_k = d_model / n_heads = <span class="number">128</span>  <span class="comment"># æ¯ä¸ªå¤´çš„ç»´åº¦</span></code></pre>

            <div class="tip"><strong>ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›</strong>
                <p style="margin-top:8px">ä¸åŒçš„æ³¨æ„åŠ›å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„ç‰¹å¾ï¼šæœ‰çš„å…³æ³¨è¯­æ³•ç»“æ„ï¼Œæœ‰çš„å…³æ³¨è¯­ä¹‰å…³ç³»ï¼Œæœ‰çš„å…³æ³¨ä½ç½®æ¨¡å¼ã€‚å¤šå¤´è®¾è®¡å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
            </div>
        </section>

        <section id="encoder" class="section">
            <h2>ç¼–ç å™¨ (Encoder)</h2>
            <p>ç¼–ç å™¨è´Ÿè´£ç†è§£è¾“å…¥åºåˆ—ï¼Œå°†å…¶è½¬æ¢ä¸ºå¯Œå«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡¨ç¤ºã€‚</p>

            <h3>ç¼–ç å™¨å±‚ç»“æ„</h3>
            <pre><code><span class="keyword">class</span> EncoderLayer(nn.Module):
    <span class="keyword">def</span> forward(self, x, mask=<span class="keyword">None</span>):
        <span class="comment"># å­å±‚ 1: Multi-Head Self-Attention + Add & Norm</span>
        attn_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + attn_output)  <span class="comment"># æ®‹å·®è¿æ¥ + LayerNorm</span>
        
        <span class="comment"># å­å±‚ 2: Feed Forward + Add & Norm</span>
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)    <span class="comment"># æ®‹å·®è¿æ¥ + LayerNorm</span>
        
        <span class="keyword">return</span> x</code></pre>

            <div class="cards">
                <div class="card">
                    <h3">ğŸ”„ Self-Attention</h3>
                        <p>Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€è¾“å…¥ï¼Œæ¯ä¸ªä½ç½®å¯ä»¥å…³æ³¨æ‰€æœ‰ä½ç½®</p>
                </div>
                <div class="card">
                    <h3">â• æ®‹å·®è¿æ¥</h3>
                        <p>x + sublayer(x)ï¼Œå¸®åŠ©æ¢¯åº¦ä¼ æ’­ï¼Œè®­ç»ƒæ›´æ·±ç½‘ç»œ</p>
                </div>
                <div class="card">
                    <h3">ğŸ“ Layer Norm</h3>
                        <p>åœ¨ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹</p>
                </div>
            </div>
        </section>

        <section id="decoder" class="section">
            <h2>è§£ç å™¨ (Decoder)</h2>
            <p>è§£ç å™¨è´Ÿè´£ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œä½¿ç”¨è‡ªå›å½’æ–¹å¼é€ä¸ªç”Ÿæˆ tokenã€‚</p>

            <h3>è§£ç å™¨å±‚ç»“æ„</h3>
            <pre><code><span class="keyword">class</span> DecoderLayer(nn.Module):
    <span class="keyword">def</span> forward(self, x, encoder_output, src_mask, tgt_mask):
        <span class="comment"># å­å±‚ 1: Masked Self-Attention</span>
        attn1 = self.masked_self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + attn1)
        
        <span class="comment"># å­å±‚ 2: Cross-Attention (å…³æ³¨ç¼–ç å™¨è¾“å‡º)</span>
        attn2 = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + attn2)
        
        <span class="comment"># å­å±‚ 3: Feed Forward</span>
        ff_output = self.feed_forward(x)
        x = self.norm3(x + ff_output)
        
        <span class="keyword">return</span> x</code></pre>

            <h3>Masked Self-Attention</h3>
            <p>åœ¨è§£ç å™¨ä¸­ï¼Œä¸ºäº†ä¿æŒè‡ªå›å½’ç‰¹æ€§ï¼Œéœ€è¦ä½¿ç”¨æ©ç é˜²æ­¢å½“å‰ä½ç½®çœ‹åˆ°æœªæ¥ä½ç½®ï¼š</p>

            <pre><code><span class="comment"># åˆ›å»ºä¸‹ä¸‰è§’æ©ç </span>
<span class="comment"># ä½ç½® i åªèƒ½çœ‹åˆ°ä½ç½® 0, 1, ..., i</span>
mask = torch.tril(torch.ones(seq_len, seq_len))
<span class="comment">"""
[1, 0, 0, 0]
[1, 1, 0, 0]
[1, 1, 1, 0]
[1, 1, 1, 1]
"""</span>

<span class="comment"># åœ¨è®¡ç®— attention æ—¶åº”ç”¨æ©ç </span>
scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="keyword">float</span>(<span class="string">'-inf'</span>))</code></pre>
        </section>

        <section id="components" class="section">
            <h2>æ ¸å¿ƒç»„ä»¶è¯¦è§£</h2>

            <h3>ä½ç½®ç¼–ç  (Positional Encoding)</h3>
            <p>Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦é¢å¤–ç¼–ç ä½ç½®ä¿¡æ¯ã€‚åŸå§‹è®ºæ–‡ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å‡½æ•°ï¼š</p>

            <div class="formula">PE(pos, 2i) = sin(pos / 10000<sup>2i/d</sup>)<br>PE(pos, 2i+1) = cos(pos /
                10000<sup>2i/d</sup>)</div>

            <pre><code><span class="keyword">def</span> positional_encoding(max_len, d_model):
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)
    div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * 
                         -(<span class="keyword">math</span>.log(<span class="number">10000.0</span>) / d_model))
    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># å¶æ•°ç»´</span>
    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># å¥‡æ•°ç»´</span>
    <span class="keyword">return</span> pe</code></pre>

            <h3>å‰é¦ˆç½‘ç»œ (Feed Forward Network)</h3>
            <pre><code><span class="comment"># ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œä¸­é—´å±‚ç»´åº¦é€šå¸¸æ˜¯ 4 å€</span>
FFN(x) = max(<span class="number">0</span>, xÂ·Wâ‚ + bâ‚)Â·Wâ‚‚ + bâ‚‚

<span class="comment"># æˆ–ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•° (GPT-2+)</span>
FFN(x) = GELU(xÂ·Wâ‚ + bâ‚)Â·Wâ‚‚ + bâ‚‚</code></pre>

            <h3>Layer Normalization</h3>
            <pre><code><span class="comment"># LayerNorm åœ¨ç‰¹å¾ç»´åº¦è®¡ç®—å‡å€¼å’Œæ–¹å·®</span>
LayerNorm(x) = Î³ Â· (x - Î¼) / Ïƒ + Î²

<span class="comment"># ä¸ BatchNorm çš„åŒºåˆ«ï¼š</span>
<span class="comment"># BatchNorm: åœ¨ batch ç»´åº¦å½’ä¸€åŒ–</span>
<span class="comment"># LayerNorm: åœ¨ feature ç»´åº¦å½’ä¸€åŒ– (é€‚åˆå˜é•¿åºåˆ—)</span></code></pre>
        </section>

        <section id="math" class="section">
            <h2>æ•°å­¦åŸç†</h2>

            <h3>ç»´åº¦å˜åŒ–</h3>
            <table>
                <thead>
                    <tr>
                        <th>æ­¥éª¤</th>
                        <th>è¾“å…¥ç»´åº¦</th>
                        <th>è¾“å‡ºç»´åº¦</th>
                        <th>è¯´æ˜</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Input Tokens</td>
                        <td>[B, L]</td>
                        <td>[B, L, D]</td>
                        <td>è¯åµŒå…¥</td>
                    </tr>
                    <tr>
                        <td>Q, K, V æŠ•å½±</td>
                        <td>[B, L, D]</td>
                        <td>[B, H, L, D/H]</td>
                        <td>å¤šå¤´æ‹†åˆ†</td>
                    </tr>
                    <tr>
                        <td>Attention Score</td>
                        <td>[B, H, L, D/H]</td>
                        <td>[B, H, L, L]</td>
                        <td>QK<sup>T</sup></td>
                    </tr>
                    <tr>
                        <td>Attention Output</td>
                        <td>[B, H, L, L]</td>
                        <td>[B, H, L, D/H]</td>
                        <td>ä¸ V ç›¸ä¹˜</td>
                    </tr>
                    <tr>
                        <td>Concat + æŠ•å½±</td>
                        <td>[B, H, L, D/H]</td>
                        <td>[B, L, D]</td>
                        <td>åˆå¹¶å¤šå¤´</td>
                    </tr>
                    <tr>
                        <td>FFN</td>
                        <td>[B, L, D]</td>
                        <td>[B, L, D]</td>
                        <td>éçº¿æ€§å˜æ¢</td>
                    </tr>
                </tbody>
            </table>
            <p style="font-size:13px;color:var(--text2)">B=Batch Size, L=Sequence Length, D=Model Dimension, H=Number of
                Heads</p>

            <h3>è®¡ç®—å¤æ‚åº¦</h3>
            <table>
                <thead>
                    <tr>
                        <th>æ“ä½œ</th>
                        <th>æ—¶é—´å¤æ‚åº¦</th>
                        <th>ç©ºé—´å¤æ‚åº¦</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Self-Attention</td>
                        <td>O(LÂ²Â·D)</td>
                        <td>O(LÂ² + LÂ·D)</td>
                    </tr>
                    <tr>
                        <td>Feed Forward</td>
                        <td>O(LÂ·DÂ²)</td>
                        <td>O(LÂ·D)</td>
                    </tr>
                    <tr>
                        <td>æ€»è®¡ (æ¯å±‚)</td>
                        <td>O(LÂ²Â·D + LÂ·DÂ²)</td>
                        <td>O(LÂ² + LÂ·D)</td>
                    </tr>
                </tbody>
            </table>

            <div class="tip warn"><strong>âš ï¸ äºŒæ¬¡å¤æ‚åº¦é—®é¢˜</strong>
                <p style="margin-top:8px">Self-Attention çš„ O(LÂ²) å¤æ‚åº¦é™åˆ¶äº†å¤„ç†è¶…é•¿åºåˆ—çš„èƒ½åŠ›ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè®¸å¤šæ¨¡å‹æœ‰ 2K-8K çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚å„ç§æ”¹è¿›å¦‚ Flash
                    Attentionã€Sparse Attention ç­‰éƒ½åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>
            </div>
        </section>

        <section id="variants" class="section">
            <h2>Transformer å˜ä½“</h2>

            <table>
                <thead>
                    <tr>
                        <th>æ¶æ„</th>
                        <th>ç±»å‹</th>
                        <th>ä»£è¡¨æ¨¡å‹</th>
                        <th>ç‰¹ç‚¹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Encoder-Only</strong></td>
                        <td>åŒå‘ç†è§£</td>
                        <td>BERT, RoBERTa</td>
                        <td>NLU ä»»åŠ¡ï¼Œåˆ†ç±»ã€NER</td>
                    </tr>
                    <tr>
                        <td><strong>Decoder-Only</strong></td>
                        <td>è‡ªå›å½’ç”Ÿæˆ</td>
                        <td>GPT ç³»åˆ—</td>
                        <td>æ–‡æœ¬ç”Ÿæˆï¼Œå¯¹è¯</td>
                    </tr>
                    <tr>
                        <td><strong>Encoder-Decoder</strong></td>
                        <td>åºåˆ—åˆ°åºåˆ—</td>
                        <td>T5, BART</td>
                        <td>ç¿»è¯‘ï¼Œæ‘˜è¦</td>
                    </tr>
                </tbody>
            </table>

            <h3>é‡è¦æ”¹è¿›</h3>
            <div class="cards">
                <div class="card">
                    <h3">âš¡ Flash Attention</h3>
                        <p>IO-aware Attentionï¼Œå‡å°‘ HBM è®¿é—®ï¼Œé€Ÿåº¦æå‡ 2-4 å€</p>
                </div>
                <div class="card">
                    <h3">ğŸŒ€ Rotary Position (RoPE)</h3>
                        <p>æ—‹è½¬ä½ç½®ç¼–ç ï¼Œæ›´å¥½çš„å¤–æ¨èƒ½åŠ›</p>
                </div>
                <div class="card">
                    <h3">ğŸ”§ RMSNorm</h3>
                        <p>ç®€åŒ–çš„ LayerNormï¼Œè®¡ç®—æ›´å¿«</p>
                </div>
                <div class="card">
                    <h3">ğŸšª SwiGLU</h3>
                        <p>GLU å˜ä½“æ¿€æ´»å‡½æ•°ï¼Œæ€§èƒ½æ›´å¥½</p>
                </div>
            </div>

            <h3>ç°ä»£ LLM æ¶æ„ (ä»¥ LLaMA ä¸ºä¾‹)</h3>
            <pre><code><span class="comment"># LLaMA çš„å…³é”®æ”¹è¿›</span>
- Pre-Norm: åœ¨ attention/FFN ä¹‹å‰åš LayerNorm
- RMSNorm: æ›¿ä»£ LayerNorm
- SwiGLU: æ›¿ä»£ ReLU/GELU
- RoPE: æ›¿ä»£ç»å¯¹ä½ç½®ç¼–ç 
- æ—  bias: çº¿æ€§å±‚å»æ‰ bias</code></pre>
        </section>

        <section id="code" class="section">
            <h2>ä»£ç å®ç°</h2>

            <h3>å®Œæ•´ Self-Attention å®ç°</h3>
            <pre><code><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> math

<span class="keyword">class</span> MultiHeadAttention(nn.Module):
    <span class="keyword">def</span> __init__(self, d_model, n_heads):
        <span class="keyword">super</span>().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        <span class="comment"># Q, K, V æŠ•å½±çŸ©é˜µ</span>
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    <span class="keyword">def</span> forward(self, q, k, v, mask=<span class="keyword">None</span>):
        batch_size = q.size(<span class="number">0</span>)
        
        <span class="comment"># çº¿æ€§æŠ•å½± + å¤šå¤´æ‹†åˆ†</span>
        Q = self.W_q(q).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)
        K = self.W_k(k).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)
        V = self.W_v(v).view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)
        
        <span class="comment"># Scaled Dot-Product Attention</span>
        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)
        <span class="keyword">if</span> mask <span class="keyword">is not None</span>:
            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="keyword">float</span>(<span class="string">'-inf'</span>))
        attn_weights = torch.softmax(scores, dim=-<span class="number">1</span>)
        attn_output = torch.matmul(attn_weights, V)
        
        <span class="comment"># åˆå¹¶å¤šå¤´ + è¾“å‡ºæŠ•å½±</span>
        attn_output = attn_output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)
        <span class="keyword">return</span> self.W_o(attn_output)</code></pre>

            <h3>Transformer Block</h3>
            <pre><code><span class="keyword">class</span> TransformerBlock(nn.Module):
    <span class="keyword">def</span> __init__(self, d_model, n_heads, d_ff, dropout=<span class="number">0.1</span>):
        <span class="keyword">super</span>().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model)
        )
        self.dropout = nn.Dropout(dropout)
        
    <span class="keyword">def</span> forward(self, x, mask=<span class="keyword">None</span>):
        <span class="comment"># Self-Attention + Add & Norm</span>
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        <span class="comment"># Feed Forward + Add & Norm</span>
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        <span class="keyword">return</span> x</code></pre>

            <div class="tip success"><strong>âœ… å®Œæ•´æ¨¡å‹å®ç°</strong>
                <p style="margin-top:8px">æ›´å®Œæ•´çš„å®ç°å¯å‚è€ƒï¼š<br>
                    â€¢ HuggingFace Transformers åº“<br>
                    â€¢ Annotated Transformer (å“ˆä½› NLP)<br>
                    â€¢ nanoGPT (Andrej Karpathy)</p>
            </div>
        </section>
    </div>
    <script>document.querySelectorAll('.toc a').forEach(a => a.addEventListener('click', e => { e.preventDefault(); document.querySelector(a.getAttribute('href')).scrollIntoView({ behavior: 'smooth' }) }))</script>
</body>

</html>